\documentclass[11pt,a4paper,twoside]{report}

\usepackage[ngerman,USenglish]{babel} % Swap languages with \selectlanguage{...}
\usepackage[utf8]{inputenc} % to write characters (à,Ü,...) in the source
\usepackage[T1]{fontenc} % make characters (à,Ü,...) appear as such in the pdf
\usepackage{lmodern}
\usepackage{amsmath,amssymb,amsfonts} % math stuff
\usepackage{array} % better implementation of tabular and array environments
\usepackage{graphicx} % graphics inclusion
\usepackage{fancyhdr} % beautiful page headers/footers
\usepackage{emptypage} % removes header/footer from empty pages
\usepackage{microtype} % beautiful typesetting
\usepackage{verbatim} % typesetting raw text

% More packages.  Uncomment what you need:

% \usepackage{bbm} % if you need $\mathbbm{1}$
% \usepackage{mathrsfs} % if you need more math script fonts $\mathscr{A}$
% \usepackage{pifont} % more symbols, e.g., circled numbers

\usepackage{pict2e} % a better picture environment
\usepackage{tikz} % versatile drawing facilities
\usetikzlibrary{fit} % used for the example tikzpicture
\usepackage{pgfplots} % versatile plotting facilities

% \usepackage{psfrag}
% \usepackage[crop=pdfcrop,cleanup={.tex,.dvi,.ps,.pdf,.log,.aux,.idx,.toc,.out}]
% {pstool} % a pdf-compatible alternative to psfrag.

% \usepackage{amsthm} % or % \usepackage{theorem} % for theorems

% \usepackage{algorithmicx} % typesetting algorithms

% \usepackage[labelfont={md}]{subfig} % for sub-floats, e.g. sub-figures

\usepackage{url} % write URLs
\usepackage{factorGraphs} % package that has macros for factor graphs

% the last package to load:
\usepackage[bookmarksnumbered=true,hypertexnames=false]{hyperref} % hyperlinks

% Page layout
\setlength{\oddsidemargin}{2.5cm}
\setlength{\evensidemargin}{0.5cm}
\addtolength{\textheight}{0.0cm}
\addtolength{\textwidth}{0.0cm}
\addtolength{\topmargin}{0.0cm}
\setlength{\headheight}{3ex}

\renewcommand{\baselinestretch}{1.2}

%\newcommand{\vecbf}{\overrightarrow{\mathbf{\vec}}_{X_{k}}}
%\newcommand{\messF}[1]{\vec{#1}_{X_{k}}}
%\newcommand{\messF}[3]{\vec{#1}_{{#2}_{k{#3}}}}
\newcommand{\messF}[3]{\overrightarrow{#1}_{{#2}_{k{#3}}}}
\newcommand{\messB}[3]{\overleftarrow{#1}_{{#2}_{k{#3}}}}

\newcommand{\mat}[1]{\mathbf{#1}}
%\overrightarrow{\mathbf{m}}_{X_{k}}

% Here the document starts
\begin{document}
\tableofcontents
\setcounter{secnumdepth}{-1}	% Remove chapter and section numbering

\newpage
\pagestyle{plain}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-------------------------------------       The Cost Function     ------------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The Cost Function}

In this very first chapter, we derive the cost function for our PLL implementation. We will see, that the application of Gaussian message passing will come up very naturally, when it comes to minimizing this cost function. The system in which we try to track the phase can generally be described by an autonomus state space model

\begin{align}
\label{eq: state space model}
\begin{split}
	x_k &= \mat{A} x_{k-1}
	\\
	y_k &= \mat{C} x_k + Z_k.
\end{split}
\end{align}

\noindent The system parameters $A$, $C$ and $Z_k$ are given by

\begin{align*}
	&\mat{A} = R(\omega_0) = 
		\begin{bmatrix}
			\cos(\omega_0) & -\sin(\omega_0) \\
			\sin(\omega_0) & \cos(\omega_0)
		\end{bmatrix},
	\\
	&\mat{C} = 
		\begin{bmatrix}
			1 & 0
		\end{bmatrix},
	\\
	&Z_k \overset{i.i.d}{\sim} \mathcal{N}\left(0, \sigma^2\right).
\end{align*}

Now assume that at time $K$, we observed the outputs $y_1, y_2, \ldots, y_K$. With these measurements, we want to find estimates $\tilde{y}, \tilde{y}_2, \ldots, \tilde{y}_K$, that provide the best fit in terms of the squared error. Our cost function $e^2$ can therefore be described as follows

\begin{equation}
	\label{eq: cost function}
	e^2 = \sum_{k=1}^K \left(y_k - \tilde{y}_k\right)^2.
\end{equation}

This cost function is reasonable, because $Z_k$ is modeled as white Gaussian noise. The estimates $\tilde{y}_k$ however, are subject to the physical laws imposed by the state space model in equation (\ref{eq: state space model}). Hence it is enough to compute the current state estimate denoted by $\hat{x}_K$. The estimated outputs are then given by

\begin{equation}
	\tilde{y}_k = \mat{C}\mat{A}^{-(K-k)} \tilde{x}_K.
\end{equation}

\noindent Using this relation in equation (\ref{eq: cost function}) yields

\begin{equation*}
	e^2\left( \tilde{x}_K \right) = \sum_{k=1}^K \left(y_k - \mat{C}\mat{A}^{-(K-k)} \tilde{x}_K\right)^2.
\end{equation*}

Note the dependence on $\tilde{x}_K$ only. We additionally introduce a so-called decay factor $\gamma$ that puts less weight on past measurements. It is defined more formally in the next chapter. The final cost function then has the form

\begin{equation}
	\label{eq: decay cost function}
	e^2\left( \tilde{x}_K \right) = \sum_{k=1}^K  \gamma^{K-k} \left(y_k - \mat{C}\mat{A}^{-(K-k)} \tilde{x}_K\right)^2.
\end{equation}

If $\tilde{x}_K$ is optimal, equation (\ref{eq: decay cost function}) is minimal. Hence $\tilde{x}_K$ can be found by minimizing $e^2$

\begin{align}
	\nonumber
	\tilde{x}_K &= \underset{x}{\arg\min} \left( \sum_{k=1}^K  \gamma^{K-k} \left(y_k - \mat{C}\mat{A}^{-(K-k)} x\right)^2 \right)\\
	\nonumber
	&= \underset{x}{\arg\max} \left( \prod_{k=1}^K  e^{-\gamma^{K-k} \left(y_k - \mat{C}\mat{A}^{-(K-k)} x\right)^2} \right)\\
	\nonumber
	&= \underset{x}{\arg\max} \left( \prod_{k=1}^K  e^{- \left(\sqrt{\gamma}^{K-k} y_k - \mat{C} \left(\sqrt{\gamma}\mat{A}\right)^{-(K-k)} x\right)^2} \right)\\
	\label{eq: minimization problem}
	&= \underset{x}{\arg\max} \left( \prod_{k=1}^K  e^{- \frac{z_k^2}{2\sigma^2}} \right),
\end{align}

\noindent where we used the noisy output relation of equation (\ref{eq: state space model}). On closer examination of equation (\ref{eq: minimization problem}), it turns out that our minimization problem can be formulated in terms of Gaussian sum-product message passing. The corresponding factor graph looks as depicted in Figure \ref{PLL_factor_graph}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\def\x{0}
		\def\y{0}
		\def\s{1}
		\def\b{1}

		\draw node[left] at (\x,\y+\s*4) {$\ldots$};
		\rightArrow{\x}{\y+\s*4}{\s*1}{$X_{k-1}$}
		\factorNode{\x+\s*1}{\y+\s*4-\b*0.5}{\b}{\b}{$\sqrt{\gamma} A$}
		\rightArrow{\x+\s*1+\b}{\y+\s*4}{\s}{}
		\factorNode{\x+\s*2+\b}{\y+\s*4-\b*0.5}{\b}{\b}{$=$}
		\rightArrow{\x+\s*2+\b*2}{\y+\s*4}{\s}{$X_{k}$}
		\draw node[right] at (\x+\s*3.25+\b*2,\y+\s*4) {$\ldots$};
	
		\downArrow{\x+\s*2+\b*1.5}{\y+\s*4-\b*0.5}{\s}{}
		\factorNode{\x+\s*2+\b}{\y+\s*3-\b*1.5}{\b}{\b}{$C$}
		\downArrow{\x+\s*2+\b*1.5}{\y+\s*3-\b*1.5}{\s}{$Y'_{k}$}
		\factorNode{\x+\s*2+\b}{\y+\s*2-\b*2.5}{\b}{\b}{$+$}
		\downArrow{\x+\s*2+\b*1.5}{\y+\s*2-\b*2.5}{\s}{}
		\observable{\x+\s*2+\b*1.375}{\y+\s-\b*2.75}{\b*0.25}{\b*0.25}{$\sqrt{\gamma}^{K-k} y_{k}$}
	
		\factorNode{\x+\s*1}{\y+\s*2-\b*2.5}{\b}{\b}{}
		\draw node[below] at (\x+\s*1+\b*0.5,\y+\s*2-\b*2.5) {$\mathcal{N}(0,\sigma^2)$};
		\rightArrow{\x+\s*1+\b}{\y+\s*2-\b*2}{\s}{$Z_{k}$}
	
		\def\x{7}

		\rightArrow{\x}{\y+\s*4}{\s*1}{$X_{K-1}$}
		\factorNode{\x+\s*1}{\y+\s*4-\b*0.5}{\b}{\b}{$\sqrt{\gamma} A$}
		\rightArrow{\x+\s*1+\b}{\y+\s*4}{\s}{}
		\factorNode{\x+\s*2+\b}{\y+\s*4-\b*0.5}{\b}{\b}{$=$}
		\rightArrow{\x+\s*2+\b*2}{\y+\s*4}{\s}{$X_{K}$}
	
		\downArrow{\x+\s*2+\b*1.5}{\y+\s*4-\b*0.5}{\s}{}
		\factorNode{\x+\s*2+\b}{\y+\s*3-\b*1.5}{\b}{\b}{$C$}
		\downArrow{\x+\s*2+\b*1.5}{\y+\s*3-\b*1.5}{\s}{$Y'_{K}$}
		\factorNode{\x+\s*2+\b}{\y+\s*2-\b*2.5}{\b}{\b}{$+$}
		\downArrow{\x+\s*2+\b*1.5}{\y+\s*2-\b*2.5}{\s}{}
		\observable{\x+\s*2+\b*1.375}{\y+\s-\b*2.75}{\b*0.25}{\b*0.25}{$y_{K}$}
	
		\factorNode{\x+\s*1}{\y+\s*2-\b*2.5}{\b}{\b}{}
		\draw node[below] at (\x+\s*1+\b*0.5,\y+\s*2-\b*2.5) {$\mathcal{N}(0,\sigma^2)$};
		\rightArrow{\x+\s*1+\b}{\y+\s*2-\b*2}{\s}{$Z_{K}$}
	\end{tikzpicture}

  	\caption[Factor graph representation of minimization problem.]
   	{Factor graph representation of minimization problem in equation \ref{eq: decay cost function}.}
	\label{PLL_factor_graph}
\end{figure}


With forward message passing on this factor graph, we can find the estimate $\hat{x}_K$ at time $K$ that minimizes the squared error $e^2\left( \tilde{x}_K \right)$. Calculation of the involved forward messages is done in the next chapter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------       Update Rules     ---------------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Derivation of Update Rules for Kalman-Based PLL}

In order to facilitate the computations, we set $\gamma=1$. It will be reintroduced later. In Figure \ref{factor_graph} we can see a factor graph representation of a Kalman-based phase-locked loop where $\gamma = 1$.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\def\x{0}
		\def\y{0}
		\def\s{1}
		\def\b{1}

		\draw node[left] at (\x,\y+\s*4) {$\ldots$};
		\rightArrow{\x}{\y+\s*4}{\s*1}{$X_{k-1}$}
		\factorNode{\x+\s*1}{\y+\s*4-\b*0.5}{\b}{\b}{$A$}
		\rightArrow{\x+\s*1+\b}{\y+\s*4}{\s}{}
		\factorNode{\x+\s*2+\b}{\y+\s*4-\b*0.5}{\b}{\b}{$=$}
		\rightArrow{\x+\s*2+\b*2}{\y+\s*4}{\s}{$X_{k}$}
		\draw node[right] at (\x+\s*3.25+\b*2,\y+\s*4) {$\ldots$};
	
		\downArrow{\x+\s*2+\b*1.5}{\y+\s*4-\b*0.5}{\s}{}
		\factorNode{\x+\s*2+\b}{\y+\s*3-\b*1.5}{\b}{\b}{$C$}
		\downArrow{\x+\s*2+\b*1.5}{\y+\s*3-\b*1.5}{\s}{$Y'_{k}$}
		\factorNode{\x+\s*2+\b}{\y+\s*2-\b*2.5}{\b}{\b}{$+$}
		\downArrow{\x+\s*2+\b*1.5}{\y+\s*2-\b*2.5}{\s}{}
		\observable{\x+\s*2+\b*1.375}{\y+\s-\b*2.75}{\b*0.25}{\b*0.25}{$y_{k}$}

		\factorNode{\x+\s*1}{\y+\s*2-\b*2.5}{\b}{\b}{}
		\draw node[below] at (\x+\s*1+\b*0.5,\y+\s*2-\b*2.5) {$\mathcal{N}(0,\sigma^2)$};
		\rightArrow{\x+\s*1+\b}{\y+\s*2-\b*2}{\s}{$Z_{k}$}
	\end{tikzpicture}

  	\caption[Factor graph representation of a Kalman filter's \textit{k}th cell.]
   	{Factor graph representation of a Kalman filter's \textit{k}th cell.}
	\label{factor_graph}
\end{figure}

Since $Z_k$ is a Gaussian random variable it follows that all random variables in the factor graph are Gaussian. As stated before, this gives rise to Gaussian message passing, i.e., all messages in the factor graph can be described by their mean vectors and covariance matrices.

We start the message passing algorithm by computing the message given by the observed samples $y_k$ \cite{SIP}

\begin{align}
	\label{eq: noise}
	\messF{m}{Z}{} = 0  \qquad &\messF{V}{Z}{} = \sigma^2 \\
	\label{eq: sample}
	\messB{m}{Y}{} = y_k  \qquad &\messB{V}{Y}{} = 0.
\end{align}

In a next step, we add the noise in Eq. (\ref{eq: noise}) to the observed sample in Eq. (\ref{eq: sample}) to get the message at $Y'_k$
\begin{align}
	\label{eq: noisy samples}
	\messB{m}{Y'}{} = y_k \qquad &\messB{V}{Y'}{} =  \sigma^2.
\end{align}

With these results, we can now compute the messages at the equality constraint
\begin{align}
	\label{eq: previous state}
	\messF{m}{X''}{} = \mat{A}\messF{m}{X}{-1} \qquad &\messF{V}{X''}{} = \mat{A}\messF{V}{X}{-1}\mat{A}^T \\
	\label{eq: input}
	\messB{W}{X'}{} = \mat{C}^T\messB{W}{Y'}{}\mat{C}  \qquad &\messB{W}{X'}{}\messB{m}{X'}{} = \mat{C}^T\messB{W}{Y'}{}\messB{m}{Y'}{},
\end{align}

where $\messB{W}{X'}{}$ denotes the precision matrix with the following equality

\begin{equation*}
	\messF{W}{X}{}^{-1} = \messF{V}{X}{},
\end{equation*}

and $\messB{W}{X'}{}\messB{m}{X'}{}$ denotes the weighted mean. The set of equations in (\ref{eq: input}) can be simplified by using the set of equations in (\ref{eq: noisy samples})

\begin{equation*}
	\messB{W}{X'}{} = \mat{C}^T\frac{1}{\sigma^2}\mat{C}  \qquad \messB{W}{X'}{}\messB{m}{X'}{} = \mat{C}^T\frac{1}{\sigma^2}y_k.
\end{equation*}


Hence we can characterize $X_k$ by its precision matrix and its weighted mean (update rules). Note that $\mat{A}$ is an invertible matrix since 

\begin{align}
  \label{eq: precision matrix}
  \messF{W}{X}{} &= \messF{W}{X''}{} + \messB{W}{X'}{} \\
  &= \left(\mat{A}\messF{V}{X}{-1}\mat{A}^T\right)^{-1} + \mat{C}^T\frac{1}{\sigma^2}\mat{C} \\
  & = \left(\mat{A}\messF{V}{X}{-1}\mat{A}^T\right)^{-1} + \frac{1}{\sigma^2}\mat{C}^T\mat{C} \\
  & = \mat{A}^{-T}\messF{W}{X}{-1}\mat{A}^{-1} + \frac{\mat{C}^T\mat{C}}{\sigma^2}  \\
  & = \mat{A}\messF{W}{X}{-1}\mat{A}^{-1} + \frac{\mat{C}^T\mat{C}}{\sigma^2},
\end{align}

\begin{align}
  \label{eq: weighted mean}
  \messF{W}{X}{}\messF{m}{X}{} &= \messF{W}{X''}{}\messF{m}{X''}{} + \messF{W}{X'}{}\messF{m}{X'}{} = \\
  &= \left(\mat{A}\messF{V}{X}{-1}\mat{A}^T\right)^{-1}\mat{A}\messF{m}{X}{-1} + \mat{C}^T\frac{1}{\sigma^2}y_k \\
  &= \left(\messF{V}{X}{-1}\mat{A}^T\right)^{-1}\underbrace{\mat{A}^{-1}\mat{A}}_{\mat{I}}\messF{m}{X}{-1} + \mat{C}^T\frac{1}{\sigma^2}y_k \\
  &= \mat{A}^{-T}\messF{W}{X}{-1}\messF{m}{X}{-1} + \frac{\mat{C}^T}{\sigma^2}y_k \\
  &= \mat{A}\messF{W}{X}{-1}\messF{m}{X}{-1} + \frac{\mat{C}^T}{\sigma^2}y_k,
\end{align}

where the last step in the particular update equations follows from the fact that $\mat{A}$ is the rotation matrix and therefore an orthogonal matrix, i.e., its transpose is equal to its inverse

\begin{equation*}
	\mat{A}^T = \mat{A}^{-1}.
\end{equation*}

What is still missing in this implementation, is the ability of our estimator to react to abrupt signal changes, that are not modeled with our state space model. Such changes serve the purpose to physically transmit information with a waveform (e.g., Phase Shift Keying (PSK)). This adaptive ability is modeled with the \emph{decay factor} $\gamma \in (0,1)$ that we've already met in the previous chapter. In every iteration step $k$, $\messF{W}{X}{-1}$ is scaled with $\gamma$ which increases the uncertainty of  past estimates and therefore puts more emphasis on new measurements. The new update rules are then given by


\begin{equation}
  \label{eq: decay precision matrix}
  \messF{W}{X}{} = \gamma \mat{A}\messF{W}{X}{-1}\mat{A}^{-1} + \frac{\mat{C}^T\mat{C}}{\sigma^2},
\end{equation} 
and

\begin{equation}
  \label{eq: decay weighted mean}
  \messF{W}{X}{}\messF{m}{X}{} = \gamma \mat{A}\messF{W}{X}{-1}\messF{m}{X}{-1} + \frac{\mat{C}^T}{\sigma^2}y_k.
\end{equation}


In a next step we try to get to an expression for the covariance matrix and the mean vector by using the Matrix Inversion Lemma (\ref{eq: matrix inversion lemma})
 
\begin{align}
	\label{eq: matrix inversion lemma}
	\left(\mat{B} + \mat{DEF}\right)^{-1} = \mat{B}^{-1} - \mat{B}^{-1}\mat{D}\left(\mat{E}^{-1} + \mat{F}\mat{B}^{-1}\mat{D}\right)^{-1}\mat{F}\mat{B}^{-1}.
\end{align}

\noindent Thus we get the following assignments from equation (\ref{eq: decay precision matrix})

\begin{align}
	\label{eq: assignments MIL}
	& \mat{B} = \gamma \mat{A}^{-T}\messF{W}{X}{-1}\mat{A}^{-1}, \\
	& \mat{D} = \mat{C}^T, \\
	& \mat{E} = \frac{1}{\sigma^2}, \\
	& \mat{F} = \mat{C}.
\end{align}

Using these, the inverse of equation (\ref{eq: precision matrix}), i.e., the covariance matrix can be written as

\begin{align}
	\label{eq: update rule, covariance}
	\messF{V}{X}{} &= \left[\gamma \mat{A}^{-T}\messF{W}{X}{-1}\mat{A}^{-1} + \frac{\mat{C}^T\mat{C}}{\sigma^2}\right]^{-1} \\
	\nonumber
	&= \left(\gamma \mat{A}^{-T}\messF{W}{X}{-1}\mat{A}^{-1}\right)^{-1} \\
	& - \left(\gamma \mat{A}^{-T}\messF{W}{X}{-1}\mat{A}^{-1}\right)^{-1}\mat{C}^T\left(\sigma^2 + \mat{C}\left(\gamma \mat{A}^{-T}\messF{W}{X}{-1}\mat{A}^{-1}\right)^{-1}\mat{C}^T\right)^{-1}\mat{C}\left(\gamma \mat{A}^{-T}\messF{W}{X}{-1}\mat{A}^{-1}\right)^{-1} \\
	\nonumber
	&= \frac{1}{\gamma} \mat{A}\messF{V}{X}{-1}\mat{A}^T - \frac{1}{\gamma^2} \mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T\left(\sigma^2 + \frac{1}{\gamma}  \mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T\right)^{-1}\mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T \\
	&= \frac{1}{\gamma} \mat{A}\messF{V}{X}{-1}\mat{A}^T - \frac{1}{\gamma} \mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T\underbrace{\left(\gamma \sigma^2 + \mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T\right)^{-1}}_{=:G}\mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T \\
	&= \frac{1}{\gamma}  \mat{A}\messF{V}{X}{-1}\mat{A}^T - \frac{1}{\gamma}  \mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^TG\mat{CA}\messF{V}{X}{-1}\mat{A}^T
\end{align}

\noindent where

\begin{equation*}
	G := \left(\gamma \sigma^2 + \mat{C} \mat{A}\messF{V}{X}{-1} \mat{A}^T\mat{C}^T \right)^{-1}.
\end{equation*}

The mean vector $\messF{m}{X}{}$ can be retrieved by multiplying the matrix $\messF{V}{X}{}$ with (\ref{eq: decay weighted mean})

\begin{align}
	\label{eq: update rule, mean vector}
	\messF{m}{X}{} &= \messF{V}{X}{} \left( \messF{W}{X}{}\messF{m}{X}{} \right) \\
	&= \frac{1}{\gamma} \left(\mat{A}\messF{V}{X}{-1}\mat{A}^T - \mat{A}\messF{V}{X}{-1}\mat{A}^T \mat{C}^T G\mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\right) \\
	&\nonumber \quad \cdot \left(\gamma \mat{A}^{-T}\messF{W}{X}{-1}\messF{m}{X}{-1} + \frac{\mat{C}^T}{\sigma^2}y_k \right) \\
	&= \mat{A} \underbrace{\messF{V}{X}{-1}\mat{A}^T\mat{A}^{-T}\messF{W}{X}{-1}}_{\mat{I}} \messF{m}{X}{-1} - \mat{A}\messF{V}{X}{-1}\mat{A}^T \mat{C}^T G\mat{C}\mat{A}\underbrace{\messF{V}{X}{-1}\mat{A}^T\mat{A}^{-T}\messF{W}{X}{-1}}_{\mat{I}}\messF{m}{X}{-1} \\
	&\nonumber \quad + \frac{1}{\gamma} \left(\mat{A}\messF{V}{X}{-1}\mat{A}^T - \mat{A}\messF{V}{X}{-1}\mat{A}^T \mat{C}^T G\mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\right) \cdot \left( \frac{\mat{C}^T}{\sigma^2}y_k \right) \\
	&= \mat{A} \messF{m}{X}{-1} - \mat{A}\messF{V}{X}{-1}\mat{A}^T \mat{C}^T G\mat{C}\mat{A}\messF{m}{X}{-1} \\
	&\nonumber \quad + \frac{1}{\gamma} \left(\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T - \mat{A}\messF{V}{X}{-1}\mat{A}^T \mat{C}^T G\mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T\right) \cdot \left(\frac{1}{\sigma^2}\right) \cdot y_k \\
	&= \mat{A} \messF{m}{X}{-1} - \left(\mat{A}\messF{V}{X}{-1}\mat{A}^T \mat{C}^T G\right)\mat{C}\mat{A}\messF{m}{X}{-1} \\
	&\nonumber \quad + \frac{1}{\gamma} \left(\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T\right) \underbrace{\left(\mat{I} - G\mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T\right) \cdot \left(\frac{1}{\sigma^2}\right)}_{=:\lambda} \cdot y_k.
\end{align}

The factor $\lambda$ can further be simplified to

\begin{align}
	\label{eq: lambda factor}
	\lambda &:= \left(\mat{I} - G\mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T\right) \cdot \left(\frac{1}{\sigma^2}\right) \\
	&= \left(GG^{-1} - G\mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T\right) \cdot \left(\frac{1}{\sigma^2}\right) \\
	&= G\left(G^{-1} - \mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T\right) \cdot \left(\frac{1}{\sigma^2}\right) \\
	&= G\left(\gamma \sigma^2 + \underbrace{\mat{C} \mat{A}\messF{V}{X}{-1} \mat{A}^T\mat{C}^T - \mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T}_{=0}\right) \cdot \left(\frac{1}{\sigma^2}\right) \\
	&= G\left(\gamma \sigma^2 \right) \cdot \left(\frac{1}{\sigma^2}\right) \\
	&= \gamma G,
\end{align}

where we used the fact, that $G^{-1} = \gamma \sigma^2 + \mat{C} \mat{A}\messF{V}{X}{-1} \mat{A}^T\mat{C}^T$. So we finally find the following condensed expression for $\messF{m}{X}{}$

\begin{align}
	\label{eq: update rule, mean vector}
	\messF{m}{X}{} &= \mat{A} \messF{m}{X}{-1} - \left(\mat{A}\messF{V}{X}{-1}\mat{A}^T \mat{C}^T G\right)\mat{C}\mat{A}\messF{m}{X}{-1} + \frac{1}{\gamma} \left(\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T \gamma G\right) y_k \\
	&= \mat{A} \messF{m}{X}{-1} + \mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T G \left( y_k - \mat{C}\mat{A}\messF{m}{X}{-1} \right).
\end{align}

%and where the last step (see equation (\ref{eq: update rule, orthogonal matrix})) follows from the fact that $\mat{A}$ is the rotation matrix and therefore an orthogonal matrix, i.e., its transpose is equal to its inverse
%
%\begin{equation*}
%	\mat{A}^T = \mat{A}^{-1}.
%\end{equation*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------------------------       Amplitude and Phase Estimate     -----------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Amplitude and Phase Estimate and Cost Function}
The predicted signal estimate given the measurements ${y_1,...,y_{k-1}}$ can be written as

\begin{equation*}
	\tilde{y}_k = \mat{CA}\messF{m}{X}{-1},
\end{equation*}

and the corrected estimate given the measurements ${y_1,...,y_k}$ as

\begin{equation*}
	\hat{y}_k = \mat{C}\messF{m}{X}{-1}.
\end{equation*}

The difference between the corrected and the predicted estimate can be deduced as

\begin{align}
	\hat{y}_k - \tilde{y}_k &= \mat{C}\left(\messF{m}{X}{} - \mat{A}\messF{m}{X}{-1}\right) \\
	\nonumber
	&= \mat{C}\left(\messF{m}{X}{} - \messF{m}{X}{} + \underbrace{\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^TG}_{=:\mat{B}_k}\left(y_k-\mat{CA}\messF{m}{X}{-1}\right)\right) \\
	\label{eq: corrected-predicted estimate}
	&= \mat{C}\mat{B}_k\left(y_k - \tilde{y}_k\right)
\end{align}

Since both the input signal and estimate is known to be sinusoidal the mean vector at time index k can be written as \cite{ST:Malmberg}

\begin{equation*}
	\messF{m}{X}{} = \hat{a}_k
		\begin{bmatrix}
			\cos{\left(\omega_0k + \hat{\phi}_k\right)} \\
			\sin{\left(\omega_0k + \hat{\phi}_k\right)}
		\end{bmatrix}
\end{equation*}

and subsequently

\begin{align}
	& y_k = a\cos{\left(\omega_0k + \phi\right)} + Z_k, \qquad Z_k \overset{i.i.d}{\sim} \mathcal{N}\left(0, \sigma^2\right), \\
	& \tilde{y}_k = \hat{a}_{k-1}\cos{\left(\omega_0k + \hat{\phi}_{k-1}\right)}, \\
	& \hat{y}_k = \hat{a}_k\cos{\left(\omega_0k + \hat{\phi}_k\right)}
\end{align}


Now we consider the scenario where only the phase is tracked and the amplitude is assumed to be $\hat{a}_k = \hat{a}_{k-1} = a_k$. If the phase estimation error is small such that $\left|\hat{\phi}_k - \phi\right|\ll1$ and $\left|\hat{\phi}_{k-1} - \phi\right|\ll1$, equation (\ref{eq: corrected-predicted estimate}) can be reduced to

\begin{align}
	a\left(\cos{\left(\omega_0k + \hat{\phi}_k\right)}- \cos{\left(\omega_0k + \hat{\phi}_{k-1}\right)}\right) &= 
	a\alpha_k\left(\cos{\left(\omega_0k + \hat{\phi}_k\right)}- \cos{\left(\omega_0k + \hat{\phi}_{k-1}\right)}\right) \\
	\sin{\left(\frac{\hat{\phi}_k - \hat{\phi}_{k-1}}{2}\right)} &\approx \alpha_k\sin{\left(\frac{\phi - \hat{\phi}_{k-1}}{2}\right)} \\
	\label{eq: phase diff}
	\hat{\phi}_k - \hat{\phi}_{k-1} &\approx \alpha_k\phi - \hat{\phi}_{k-1}.
\end{align}

Thus follows from equation (\ref{eq: phase diff}) that the phase update can be approximated as

\begin{equation}
	\hat{\phi}_k \approx \alpha_k\phi + \left(1-\alpha_k\right)\hat{\phi}_{k-1}.
\end{equation}


In a similar way, the amplitudes can be extracted under the condition that $\hat{\phi}_k = \hat{\phi}_{k-1} = \phi$

\begin{align}
	\hat{a}_k\cos{\left(\omega_0k + \phi\right)} - \hat{a}_{k-1}\cos{\left(\omega_ok + \phi\right)} &= \alpha_k\left(a\cos{\left(\omega_0k + \phi\right)} - \hat{a}_{k-1}\cos{\left(\omega_ok + \phi\right)}\right) \\
	\hat{a}_k - \hat{a}_{k-1} &= \alpha_k\left(a-\hat{a}_{k-1}\right) \\
	\hat{a}_k &= \alpha_ka + \left(1-\alpha_k\right)\hat{a}_{k-1}.
\end{align}

The cost function for the PLL then can be written as follows

\begin{equation}
	J = \mathbb{E}\left[\right]
\end{equation}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------       Independence of state update from noise     ------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Independence of State Update from Noise}

The update rules in Equations (\ref{eq: decay precision matrix}) and (\ref{eq: decay weighted mean}) are still iterative methods to compute the precision matrix and the weighted mean vector. Equations (\ref{eq: explicit precision matrix}) and (\ref{eq: explicit weighted mean}) provide explicit formulas to compute the $\messF{W}{X}{}$ and $\messF{W}{X}{}\messF{m}{X}{}$ respectively

\begin{equation}
	\label{eq: explicit precision matrix}
	\messF{W}{X}{} = \frac{1}{\sigma^2}\sum_{j=0}^{k-1}\gamma^{j}\left(\mat{A}^{-T}\right)^j\mat{C}^T\mat{C}\left(\mat{A}^{-1}\right)^{j},
\end{equation}

\begin{equation}
	\label{eq: explicit weighted mean}
	\messF{W}{X}{}\messF{m}{X}{} = \frac{1}{\sigma^2}\sum_{j=0}^{k-1}\gamma^{j} \left(\mat{A}^{-T}\right)^j\mat{C}^T y_{k-j}.
\end{equation}

The equality of (\ref{eq: explicit precision matrix}) and (\ref{eq: decay precision matrix}) as well as the equality of (\ref{eq: explicit weighted mean}) and (\ref{eq: decay weighted mean}) can easily be verified by induction. The independence of $\messF{m}{X}{}$ from the output noise variance $\sigma^2$ can now be proved by combining the two equations, which yields

\begin{align}
	\label{eq mean independence}
	\messF{m}{X}{} &= \left( \messF{W}{X}{}\right)^{-1} \left(\messF{W}{X}{}\messF{m}{X}{}\right) \\
	&= \left(\frac{1}{\sigma^2} \sum_{j=0}^{k-1}\gamma^{j}\left(\mat{A}^{-T}\right)^j\mat{C}^T\mat{C}\left(\mat{A}^{-1}\right)^{j}, \right)^{-1} \cdot \left( \frac{1}{\sigma^2}\sum_{j=0}^{k-1}\gamma^{j} \left(\mat{A}^{-T}\right)^j\mat{C}^T y_{k-j} \right) \\
	&= \left(\sum_{j=0}^{k-1}\gamma^{j}\left(\mat{A}^{-T}\right)^j\mat{C}^T\mat{C}\left(\mat{A}^{-1}\right)^{j}, \right)^{-1} \cdot  \left(\sum_{j=0}^{k-1}\gamma^{j} \left(\mat{A}^{-T}\right)^j\mat{C}^T y_{k-j} \right)
\end{align}

As we can see, the noise $\sigma^2$ cancels out in the calculation above, i.e. our estimator does not need any information about the Gaussian noise process.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------------------------       Steady State Precision Matrix       -----------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Steady State Precision Matrix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------------------------------       Small decay factor      ------------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ $1^{\text{st}}$ case: Decay factor $0<\gamma< 1$ }

Since $\mat{A}$ is a rotation matrix it can be diagonalized in an orthonormal basis in $\mathcal{C}$ such that

\begin{equation}
	\mat{A} = \mat{Q\Lambda}\mat{Q}^H,
\end{equation}

where $\mat{Q}$ is a unitary matrix and $\mat{Q}^H$ denotes  its Hermitian transpose. The decomposition of the matrix $\mat{A}$ can be done as follows

\begin{align*}
	&\mat{Q} = \frac{1}{\sqrt{2}}
		\begin{bmatrix}
			1 & 1 \\
			-i & i
		\end{bmatrix},
	\\
	&\mat{\Lambda} = 
		\begin{bmatrix}
			\exp{(i\omega_0)} & 0 \\
			0 & \exp{(-i\omega_0)}
		\end{bmatrix}.
\end{align*}


In a next step, the new expression for $\mat{A}$ can be inserted into equation (\ref{eq: explicit precision matrix}) \cite{ST:Malmberg}

\begin{align}
	\nonumber
	\messF{W}{X}{} &= \sum_{l=0}^{k-1}\left(\gamma^{l}\left(\left(\mat{Q\Lambda}\mat{Q}^H\right)^H\right)^{-l}\frac{\mat{C}^T\mat{C}}{\sigma^2}\left(\mat{Q\Lambda}\mat{Q}^H\right)^{-l}\right) \\
	\label{eq: update rule, unitary Q}
	&= \frac{1}{\sigma^2}\sum_{l=0}^{k-1}\left(\gamma^{l}\mat{Q}\left(\bar{\mat{\Lambda}}\right)^{-l}\mat{Q}^H
		\begin{bmatrix}
			1 & 0 \\
			0 & 0
		\end{bmatrix}
		\mat{Q}\mat{\Lambda}^{-l}\mat{Q}^H\right)
	\\
	\nonumber
	&= \frac{1}{2\sigma^2}\mat{Q}\sum_{l=0}^{k-1}\left(\gamma^{l}\left(\bar{\mat{\Lambda}}\right)^{-l}
		\begin{bmatrix}
			1 & 1 \\
			1 & 1
		\end{bmatrix}
		\mat{\Lambda}^{-l}\right)\mat{Q}^H
	\\
	\nonumber
	&= \frac{1}{2\sigma^2}\mat{Q}\sum_{l=0}^{k-1}\left(\gamma^{l}
		\begin{bmatrix}
			\exp{(i\omega_0l)} & 0 \\
			0 & \exp{(-i\omega_0l)}
		\end{bmatrix}
		\begin{bmatrix}
			1 & 1 \\
			1 & 1
		\end{bmatrix}
		\begin{bmatrix}
			\exp{(-i\omega_0l)} & 0 \\
			0 & \exp{(i\omega_0l)}
		\end{bmatrix}
		\right)\mat{Q}^H
	\\
	\nonumber
	&= \frac{1}{2\sigma^2}\mat{Q}\sum_{l=0}^{k-1}\left( \gamma^{l}
		\begin{bmatrix}
			\exp{(i\omega_0l)} & \exp{(i\omega_0l)} \\
			\exp{(-i\omega_0l)} & \exp{(-i\omega_0l)}
		\end{bmatrix}
		\begin{bmatrix}
			\exp{(-i\omega_0l)} & 0 \\
			0 & \exp{(i\omega_0l)}
		\end{bmatrix}
		\right)\mat{Q}^H
	\\
	\nonumber
	&= \frac{1}{2\sigma^2}\mat{Q}\sum_{l=0}^{k-1}\left(\gamma^{l}
		\begin{bmatrix}
			1 & \exp{(2i\omega_0l)} \\
			\exp{(-2i\omega_0l)} & 1
		\end{bmatrix}
		\right)\mat{Q}^H
	\\
	\nonumber
	&= \frac{1}{2\sigma^2}\mat{Q}\sum_{l=0}^{k-1}\left(\gamma^{l}
		\begin{bmatrix}
			1 & 0 \\
			0 & 1
		\end{bmatrix} + \gamma^{l}
		\begin{bmatrix}
			0 & \exp{(2i\omega_0l)} \\
			\exp{(-2i\omega_0l)} & 0
		\end{bmatrix}
		\right)\mat{Q}^H
	\\
	\label{eq: update rule, orthogonal basis}
	&= \frac{1}{2\sigma^2}\left(\frac{1-\gamma^k}{1-\gamma}\mat{I}_2 + \mat{Q}\sum_{l=0}^{k-1}\left(\gamma^{l}
		\begin{bmatrix}
			0 & \exp{(2i\omega_0l)} \\
			\exp{(-2i\omega_0l)} & 0
		\end{bmatrix}
		\right)\mat{Q}^H\right) ,
\end{align}

\noindent where equation (\ref{eq: update rule, unitary Q}) follows from the unitary property of $\mat{Q}$. Moreover, we used the geometric series in equation (\ref{eq: update rule, orthogonal basis}) 

\begin{equation*}
	\sum_{n=0}^{n-1}ar^k = a\frac{1-r^n}{1-r},
\end{equation*}

\noindent where we implicitly assumed that $0 < \gamma < 1$. Now we rewrite equation (\ref{eq: update rule, orthogonal basis}) as follows

\begin{align}
	\label{eq: update rule, geometric series}
	\sum_{l=0}^{k-1}\gamma^{l}\exp{\left(2i\omega_0l\right)} &= \sum_{l=0}^{k-1}\left(\gamma \exp{\left(2i\omega_0\right)}\right)^l \\
	\nonumber
	&= \frac{1-\gamma^{k}\exp{\left(2i\omega_0k\right)}}{1-\gamma \exp{\left(2i\omega_0\right)}} \\
	\nonumber
	&= \frac{\left(1-\gamma^{k}\right)+\gamma^{k}-\gamma^{k}\exp{\left(2i\omega_0k\right)}}{\left(1-\gamma\right)+\gamma-\gamma \exp{\left(2i\omega_0\right)}} \\
	\nonumber
	&= \frac{\left(1-\gamma^{k}\right)-\gamma^{k}\exp{\left(i\omega_0k\right)}\left(\exp{\left(i\omega_0k\right)}-\exp{\left(-i\omega_0k\right)}\right)}{\left(1-\gamma\right) - \gamma \exp{\left(i\omega_0\right)}\left(\exp{\left(i\omega_0\right)}-\exp{\left(-i\omega_0\right)}\right)} \\
	\label{eq: update rule, positive euler}
	&= \frac{\left(1-\gamma^{k}\right)-2\gamma^{k} \exp{\left(i\omega_0k\right)}\sin\left(\omega_0 k\right)}{\left(1-\gamma\right) - 2\gamma \exp{\left(i\omega_0\right)}\sin\left(\omega_0\right)}
\end{align}

\noindent where we made again use of the geometric series and the following identity for the sine (derived from Euler's formula) (see equation (\ref{eq: update rule, positive euler}))

\begin{equation*}
	\sin{\left(x\right)} = \frac{1}{2i}\left(e^{ix} - e^{-ix}\right).
\end{equation*}

\noindent Similarly we get

\begin{align}
\label{eq: update rule, negative euler}
	\sum_{j=0}^{k-1}\gamma^{l}\exp{\left(-2i\omega_0j\right)} &= \frac{1-\gamma^{k}\exp{\left(-2i\omega_0k\right)}}{1-\gamma \exp{\left(-2i\omega_0\right)}} \\
	&= \frac{\left(1-\gamma^{k}\right) + 2\gamma^{k} \exp{\left(-i\omega_0k\right)}\sin\left(\omega_0 k\right)}{\left(1-\gamma\right) + 2\gamma \exp{\left(-i\omega_0\right)}\sin\left(\omega_0\right)}. 
\end{align}

We are now interested in the long-term behavior of the precision matrix $\messF{W}{X}{}$, i.e., in the special case where $k \rightarrow \infty$. We will call the resulting matrix $\overrightarrow{W}_{ss}$ steady state precision matrix. It is formally defined as follows

\begin{equation*}
	 \overrightarrow{W}_{ss} := \underset{k \rightarrow \infty}{\lim}\left( \messF{W}{X}{} \right).
\end{equation*}

\noindent As $k$ tends to infinity, the second term of equation (\ref{eq: update rule, orthogonal basis}) simplifies to

\begin{align}
	\label{eq: second term}
	&\underset{k \rightarrow \infty}{\lim} \mat{Q}\sum_{l=0}^{k-1}\left(\gamma^{l}
		\begin{bmatrix}
			0 & \exp{(2i\omega_0l)} \\
			\exp{(-2i\omega_0l)} & 0
		\end{bmatrix}
		\right)\mat{Q}^H
	\\
	\nonumber
	&= \frac{1}{\sqrt{2}}
		\begin{bmatrix}
			1 & 1\\
			-i & i
		\end{bmatrix}
		\begin{bmatrix}
			0 & \left(1-\gamma \exp{\left(2i\omega_0\right)}\right)^{-1} \\
			\left(1-\gamma \exp{\left(-2i\omega_0\right)}\right)^{-1} & 0
		\end{bmatrix}
		\frac{1}{\sqrt{2}}
		\begin{bmatrix}
			1 & i \\
			1 & -i
		\end{bmatrix}
	\\
	\nonumber
	&= \frac{1}{2}
		\begin{bmatrix}
			\left(1-\gamma \exp{\left(-2i\omega_0\right)}\right)^{-1} & \left(1-\gamma \exp{\left(2i\omega_0\right)}\right)^{-1} \\
			i\left(1-\gamma \exp{\left(-2i\omega_0\right)}\right)^{-1} & -i\left(1-\gamma \exp{\left(2i\omega_0\right)}\right)^{-1}
		\end{bmatrix}
		\begin{bmatrix}
			1 & i \\
			1 & -i
		\end{bmatrix}
	\\
	\nonumber
	&= \frac{1}{2} \frac{1}{\left( 1-\gamma\exp(2i\omega_0) \right)  \left( 1-\gamma\exp(-2i\omega_0) \right)}\\
	\nonumber
	& \cdot
		\begin{bmatrix}
			2 - \gamma \left( \exp(2i\omega_0) + \exp(-2i\omega_0) \right) & -i \gamma \left( \exp(2i\omega_0) - \exp(-2i\omega_0) \right) \\
			 -i \gamma \left( \exp(2i\omega_0) - \exp(-2i\omega_0) \right) & -2 + \gamma \left( \exp(2i\omega_0) + \exp(-2i\omega_0) \right)
		\end{bmatrix}
	\\
	&= \frac{1}{2} \frac{1}{1+\gamma^2 - 2 \gamma \cos\left( 2\omega_0 \right)}
		\begin{bmatrix}
			2\left(1 - \gamma \cos\left( 2\omega_0 \right)\right) & 2 \gamma \sin\left( 2 \omega_0 \right) \\
			 2 \gamma \sin\left( 2 \omega_0 \right) &- 2\left(1 - \gamma \cos\left( 2\omega_0 \right)\right)
		\end{bmatrix} .
\end{align}

\noindent The steady state precision matrix is therefore given by

\begin{equation*}
	\overrightarrow{W}_{ss} = \frac{1}{2\sigma^2}\left(\frac{1}{1-\gamma}\mat{I}_2 + \frac{1}{1+\gamma^2 - 2 \gamma \cos\left( 2\omega_0 \right)}
		\begin{bmatrix}
			1 - \gamma \cos\left( 2\omega_0 \right)&  \gamma \sin\left( 2 \omega_0 \right) \\
			  \gamma \sin\left( 2 \omega_0 \right) &- 1 + \gamma \cos\left( 2\omega_0 \right)
		\end{bmatrix}\right) .
\end{equation*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------       Decay factor equal to 1      ---------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{ $2^{\text{nd}}$ case:  Decay factor $\gamma = 1$ }

\noindent For the special case where $\gamma = 1$, equations (\ref{eq: update rule, positive euler}) and (\ref{eq: update rule, negative euler}) simplify to

\begin{equation*}
	\label{eq: update rule, geometric series}
	\sum_{l=0}^{k-1}\exp{\left(2i\omega_0l\right)}= \exp{\left(i\omega_0(k-1)\right)}\frac{\sin\left(\omega_0 k\right)}{ \sin\left(\omega_0\right)} \\
\end{equation*}

\noindent and

\begin{equation*}
	\label{eq: update rule, geometric series}
	\sum_{l=0}^{k-1}\exp{\left(2i\omega_0l\right)}= \exp{\left(-i\omega_0(k-1)\right)}\frac{\sin\left(\omega_0 k\right)}{ \sin\left(\omega_0\right)} \\
\end{equation*}

\noindent respectively. The identical calculation as in \ref{eq: second term} then yields

\begin{align}
	\nonumber
	& \mat{Q}\sum_{l=0}^{k-1}\left(
		\begin{bmatrix}
			0 & \exp{(2i\omega_0l)} \\
			\exp{(-2i\omega_0l)} & 0
		\end{bmatrix}
		\right)\mat{Q}^H
	\\
	\nonumber
	&= \frac{\sin{\left(\omega_0k\right)}}{2\sin{\left(\omega_0\right)}}
		\begin{bmatrix}
			1 & 1\\
			-i & i
		\end{bmatrix}
		\begin{bmatrix}
			0 & \exp{\left(i\omega_0\left(k-1\right)\right)} \\
			\exp{\left(-i\omega_0\left(k-1\right)\right)} & 0
		\end{bmatrix}
		\begin{bmatrix}
			1 & i \\
			1 & -i
		\end{bmatrix}
	\\
	\nonumber
	&= \frac{\sin{\left(\omega_0k\right)}}{2\sin{\left(\omega_0\right)}}
		\begin{bmatrix}
			\exp{\left(-i\omega_0\left(k-1\right)\right)} & \exp{\left(i\omega_0\left(k-1\right)\right)} \\
			i\exp{\left(-i\omega_0\left(k-1\right)\right)} & -i\exp{\left(i\omega_0\left(k-1\right)\right)}
		\end{bmatrix}
		\begin{bmatrix}
			1 & i \\
			1 & -i
		\end{bmatrix}
	\\
	\nonumber
	&= \frac{\sin{\left(\omega_0k\right)}}{2\sin{\left(\omega_0\right)}}
		\begin{bmatrix}
			2\cos{\left(\omega_0\left(k-1\right)\right)} & 2\sin{\left(\omega_0\left(k-1\right)\right)} \\
			2\sin{\left(\omega_0\left(k-1\right)\right)} & -2\cos{\left(\omega_0\left(k-1\right)\right)}
		\end{bmatrix}
	\\
	&= \frac{\sin{\left(\omega_0k\right)}}{\sin{\left(\omega_0\right)}}\mat{A}^{k-1}\mat{S}
\end{align}

where $\mat{S} = \begin{bmatrix} 1 & 0 \\
0 & -1 \end{bmatrix}$. The precision matrix is now reduced to

\begin{equation*}
	\messF{W}{X}{} = \frac{1}{2\sigma^2}\left(k\mat{I}_2 + \frac{\sin{\left(\omega_0k\right)}}{\sin{\left(\omega_0\right)}}\mat{A}^{k-1}\mat{S}\right) .
\end{equation*}

As we can see, $\messF{W}{X}{}$ will never end up in a steady state, i.e., for $k \rightarrow \infty$, the precision matrix will get bigger with linear divergence speed. The corresponding covariance matrix looks as follows

\begin{align}
	\messF{V}{X}{} &= \left(\messF{W}{X}{}\right)^{-1} \\
	&= \frac{2\sigma^2}{k^2-\left(\frac{\sin{\left(\omega_0k\right)}}{\sin{\left(\omega_0\right)}}\right)^2}\left( k \mat{I}_2 - \frac{\sin{\left(\omega_0k\right)}}{\sin{\left(\omega_0\right)}}\mat{A}^{k-1}\mat{S}\right).
\end{align}

The Kalman gain is then defined as

\begin{align}
	\alpha_k &= \mat{CB}_k \\
	&= \mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^TG \\
	&= \mat{C}\mat{A}\messF{V}{X}{-1}\mat{A}^T\mat{C}^T\left( \sigma^2 + \mat{C} \mat{A}\messF{V}{X}{-1} \mat{A}^T\mat{C}^T \right)^{-1} \\
	&= \frac{2\left(k-\frac{\sin{\left(\omega_0k\right)}}{\sin{\left(\omega_0\right)}}\cos{\left(\omega_0\left(k+1\right)\right)}\right)}{k^2-\left(\frac{\sin{\left(\omega_0k\right)}}{\sin{\left(\omega_0\right)}}\right)^2 + 2\left(k-\frac{\sin{\left(\omega_0k\right)}}{\sin{\left(\omega_0\right)}}\cos{\left(\omega_0\left(k+1\right)\right)}\right)}.
\end{align}

\clearpage

% Bibliography
\bibliographystyle{unsrt} % bibliography style in order of first citation
%\bibliographystyle{splncs}
\bibliography{references}

\end{document}
